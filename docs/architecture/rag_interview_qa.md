# OneBook AI — RAG 面试问答（初稿）

> 用途：用于项目面试准备。该版本为初稿，你可逐条标注需要优化的回答。

## Q1. RAG 的工作机制是什么？
RAG 是“检索增强生成”：先从外部知识库检索与问题相关的证据，再把证据和问题一起交给大模型生成答案。它的核心价值是让回答有依据、可追溯，而不是只依赖模型参数中的记忆。

你的修订意见：

---

## Q2. RAG 有哪几个步骤？
标准流程可以拆成：
1. 数据准备（清洗、解析、分块、元数据）
2. 向量化与索引构建
3. 查询处理（规范化、改写、路由）
4. 检索与重排（召回 + 精排）
5. 证据约束生成
6. 评估与持续优化

你的修订意见：

---

## Q3. 选择的哪个向量数据库，为什么？
当前项目使用 Postgres + pgvector。原因是 MVP 阶段工程复杂度低，结构化数据与向量可同库管理，事务和运维链路成熟。后续数据规模上升时，可迁移到专用向量数据库。

你的修订意见：

---

## Q4. 你怎样保证检索准确性？
我从四层保证准确性：
1. 数据层：清洗、结构化分块、元数据规范
2. 召回层：混合检索（dense + BM25）
3. 排序层：两阶段检索与重排、去重
4. 生成层：证据约束与引用一致性校验

你的修订意见：

---

## Q5. 如何提高召回率和准确性？
- 提召回：查询改写、多查询召回、提升初始 TopN
- 提准确：重排、元数据过滤、上下文去重打包、证据约束生成
实践策略是“先高召回，再高精排”。

你的修订意见：

---

## Q6. 你在实际项目中用了什么优化技巧？标准流程是什么？
我在问答链路里的优化重点是“先保证召回，再提升排序，最后约束生成”：

1. 查询侧优化：对用户问题做规范化、术语归一、查询改写；复杂问题做多查询召回，减少“问法差异”导致的漏召回。  
2. 召回侧优化：从单路向量检索升级为混合检索（dense + BM25），同时保留元数据过滤能力。  
3. 精排侧优化：使用两阶段检索（TopN 召回 + rerank 重排），并做去重与上下文打包，提升上下文有效密度。  
4. 生成侧优化：强制基于证据回答，要求引用；证据不足时拒答，降低幻觉。  
5. 后校验优化：对答案做 groundedness/引用一致性检查，不通过则降级或重试。  

标准流程是：先做高召回（Recall），再做高精度（Precision/nDCG），最后做真实性约束（Faithfulness）。

你的修订意见：

---

## Q7. 你的 RAG 怎么做效果评估？
我用“三层评估 + 双数据源”的方式做效果评估：

1. 检索层（Retriever）  
   - 指标：Recall@K、nDCG@K、MRR  
   - 目标：评估“能不能召回 + 排名是否合理”。

2. 生成层（Generator）  
   - 指标：答案正确率、引用命中率、幻觉率、拒答正确率  
   - 目标：评估“答得对不对、有没有证据、该拒答时是否拒答”。

3. 工程层（System）  
   - 指标：P95 延迟、失败率、单次问答成本  
   - 目标：评估“线上是否可用、可控、可持续”。

数据上我会同时看：  
- 固定离线评测集：做版本对比，保证可复现。  
- 线上抽样回流集：覆盖真实用户问法与边界 case。  

核心原则是：离线指标提升但线上指标恶化，不算优化成功。

你的修订意见：

---

## Q8. 你认为 RAG 为什么还会出现幻觉？怎么解决？
RAG 仍会幻觉，核心不是“模型太差”，而是“证据链断了”。常见断点有四类：

1. 没召回到关键证据  
   - 原因：问题表达与语料表达不一致、召回策略单一。  
   - 方案：查询改写 + 多查询召回 + 混合检索。

2. 召回到了，但排序不对  
   - 原因：噪声 chunk 排在前面，关键证据被淹没。  
   - 方案：两阶段检索（TopN + rerank）+ 去重打包。

3. 证据够，但生成没被约束  
   - 原因：提示词允许模型自由补全。  
   - 方案：强制证据约束输出、必须引用来源。

4. 有答案但无证据校验  
   - 原因：缺少输出后检查。  
   - 方案：groundedness / 引用一致性校验，不通过就拒答或重试。

一句话总结：先修检索，再加约束，最后加校验，三层一起做才能真正降幻觉。

你的修订意见：

---

## Q9. 边界 case 你怎么解决？
我一般先把边界 case 分成“输入问题、检索问题、状态问题、输出问题”四类，再分别兜底：

1. 输入问题：扫描版 PDF、低质量文本、格式噪声  
   - 做法：先走 native 提取，再按页质量触发 OCR；对文本做清洗和标准化。  
   - 目的：减少“源数据不可读”导致的后续误检索。

2. 检索问题：用户问法很短、很口语、和语料措辞差异大  
   - 做法：query normalize + 查询改写 + 多查询召回；再用重排去噪。  
   - 目的：减少漏召回和错召回。

3. 状态问题：书籍还在 queued/processing，用户已发起问答  
   - 做法：在服务层做状态门禁，未 ready 直接返回可解释提示，不进入生成链路。  
   - 目的：避免“无索引情况下强行回答”。

4. 输出问题：有回答但缺证据，或证据不足  
   - 做法：强制引用；证据不足时标准化拒答；必要时触发降级策略。  
   - 目的：控制幻觉并保证可追溯性。

一句话：边界 case 不是补丁处理，而是按链路分层设置护栏。

你的修订意见：

---

## Q10. 项目落地有哪些痛点，你如何解决？
这个项目落地时我遇到的痛点主要有三类：

1. 数据质量不稳定（尤其 PDF）  
   - 痛点：同是 PDF，文本层质量差异很大，直接影响分块和检索。  
   - 解法：建立“native 提取 + OCR 回退 + 页级质量融合”策略，并把提取方式写入 metadata 做追溯。

2. 质量和性能的冲突  
   - 痛点：加改写、多查询、重排会提升质量，但也会拉高延迟和成本。  
   - 解法：采用两阶段检索和参数化预算（TopN、TopK、重排开关），用 P95 与成本做门禁。

3. 优化不可验证、上线风险高  
   - 痛点：很多优化“看起来有效”，但上线后不稳定。  
   - 解法：固定离线评测集 + 线上抽样回流；策略开关化；灰度发布与回滚预案。

所以我的落地思路是：先把数据护栏和证据链建稳，再做检索提准，最后用评测和门禁保证可持续迭代。

你的修订意见：

---

## 备注
- 该文档是面试回答初稿，不等同于正式架构方案。
- 目标是让回答“可落地、可量化、可追问”。
